{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5deb796e",
   "metadata": {},
   "source": [
    "# Banking: Credit Card Default"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2fd0072",
   "metadata": {},
   "source": [
    "### Collecting a Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be24aaa6",
   "metadata": {},
   "source": [
    "#### Datset\n",
    "- https://www.kaggle.com/datasets/uciml/default-of-credit-card-clients-dataset?select=UCI_Credit_Card.csv\n",
    "- https://www.kaggle.com/code/muskanbhasin/credit-card-default-prediction\n",
    "\n",
    "#### Original Datasets\n",
    "- https://archive.ics.uci.edu/ml/datasets/default+of+credit+card+clients#\n",
    "- https://archive.ics.uci.edu/ml/machine-learning-databases/00350/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f63847",
   "metadata": {},
   "source": [
    "### Content\n",
    "##### There are 25 variables:\n",
    "\n",
    "- ID: ID of each client\n",
    "- LIMIT_BAL: Amount of given credit in NT dollars (includes individual and family/supplementary credit\n",
    "- SEX: Gender (1=male, 2=female)\n",
    "- EDUCATION: (1=graduate school, 2=university, 3=high school, 4=others, 5=unknown, 6=unknown)\n",
    "- MARRIAGE: Marital status (1=married, 2=single, 3=others)\n",
    "- AGE: Age in years\n",
    "- PAY_0: Repayment status in September, 2005 (-1=pay duly, 1=payment delay for one month, 2=payment delay for two months, … 8=payment delay for eight months, 9=payment delay for nine months and above)\n",
    "- PAY_2: Repayment status in August, 2005 (scale same as above)\n",
    "- PAY_3: Repayment status in July, 2005 (scale same as above)\n",
    "- PAY_4: Repayment status in June, 2005 (scale same as above)\n",
    "- PAY_5: Repayment status in May, 2005 (scale same as above)\n",
    "- PAY_6: Repayment status in April, 2005 (scale same as above)\n",
    "- BILL_AMT1: Amount of bill statement in September, 2005 (NT dollar)\n",
    "- BILL_AMT2: Amount of bill statement in August, 2005 (NT dollar)\n",
    "- BILL_AMT3: Amount of bill statement in July, 2005 (NT dollar)\n",
    "- BILL_AMT4: Amount of bill statement in June, 2005 (NT dollar)\n",
    "- BILL_AMT5: Amount of bill statement in May, 2005 (NT dollar)\n",
    "- BILL_AMT6: Amount of bill statement in April, 2005 (NT dollar)\n",
    "- PAY_AMT1: Amount of previous payment in September, 2005 (NT dollar)\n",
    "- PAY_AMT2: Amount of previous payment in August, 2005 (NT dollar)\n",
    "- PAY_AMT3: Amount of previous payment in July, 2005 (NT dollar)\n",
    "- PAY_AMT4: Amount of previous payment in June, 2005 (NT dollar)\n",
    "- PAY_AMT5: Amount of previous payment in May, 2005 (NT dollar)\n",
    "- PAY_AMT6: Amount of previous payment in April, 2005 (NT dollar)\n",
    "- default.payment.next.month: Default payment (1=yes, 0=no)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff33ec0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "dataset = \"D:\\\\iNeuron\\internship_projects\\\\Credit Card Default Prediction\"\n",
    "file_name = \"UCI_Credit_Card.csv\"\n",
    "cwd = os.getcwd()\n",
    "\n",
    "file_path = os.path.join(cwd,dataset,file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e658186b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install opendatasets\n",
    "# !pip install seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd2f1d24",
   "metadata": {},
   "source": [
    "## importing Lib and packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c8f2b1cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb27039c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>LIMIT_BAL</th>\n",
       "      <th>SEX</th>\n",
       "      <th>EDUCATION</th>\n",
       "      <th>MARRIAGE</th>\n",
       "      <th>AGE</th>\n",
       "      <th>PAY_0</th>\n",
       "      <th>PAY_2</th>\n",
       "      <th>PAY_3</th>\n",
       "      <th>PAY_4</th>\n",
       "      <th>...</th>\n",
       "      <th>BILL_AMT4</th>\n",
       "      <th>BILL_AMT5</th>\n",
       "      <th>BILL_AMT6</th>\n",
       "      <th>PAY_AMT1</th>\n",
       "      <th>PAY_AMT2</th>\n",
       "      <th>PAY_AMT3</th>\n",
       "      <th>PAY_AMT4</th>\n",
       "      <th>PAY_AMT5</th>\n",
       "      <th>PAY_AMT6</th>\n",
       "      <th>default.payment.next.month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>20000.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>689.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>120000.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>26</td>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>3272.0</td>\n",
       "      <td>3455.0</td>\n",
       "      <td>3261.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>90000.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>14331.0</td>\n",
       "      <td>14948.0</td>\n",
       "      <td>15549.0</td>\n",
       "      <td>1518.0</td>\n",
       "      <td>1500.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>50000.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>28314.0</td>\n",
       "      <td>28959.0</td>\n",
       "      <td>29547.0</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>1100.0</td>\n",
       "      <td>1069.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>50000.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>57</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>20940.0</td>\n",
       "      <td>19146.0</td>\n",
       "      <td>19131.0</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>36681.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>9000.0</td>\n",
       "      <td>689.0</td>\n",
       "      <td>679.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID  LIMIT_BAL  SEX  EDUCATION  MARRIAGE  AGE  PAY_0  PAY_2  PAY_3  PAY_4  \\\n",
       "0   1    20000.0    2          2         1   24      2      2     -1     -1   \n",
       "1   2   120000.0    2          2         2   26     -1      2      0      0   \n",
       "2   3    90000.0    2          2         2   34      0      0      0      0   \n",
       "3   4    50000.0    2          2         1   37      0      0      0      0   \n",
       "4   5    50000.0    1          2         1   57     -1      0     -1      0   \n",
       "\n",
       "   ...  BILL_AMT4  BILL_AMT5  BILL_AMT6  PAY_AMT1  PAY_AMT2  PAY_AMT3  \\\n",
       "0  ...        0.0        0.0        0.0       0.0     689.0       0.0   \n",
       "1  ...     3272.0     3455.0     3261.0       0.0    1000.0    1000.0   \n",
       "2  ...    14331.0    14948.0    15549.0    1518.0    1500.0    1000.0   \n",
       "3  ...    28314.0    28959.0    29547.0    2000.0    2019.0    1200.0   \n",
       "4  ...    20940.0    19146.0    19131.0    2000.0   36681.0   10000.0   \n",
       "\n",
       "   PAY_AMT4  PAY_AMT5  PAY_AMT6  default.payment.next.month  \n",
       "0       0.0       0.0       0.0                           1  \n",
       "1    1000.0       0.0    2000.0                           1  \n",
       "2    1000.0    1000.0    5000.0                           0  \n",
       "3    1100.0    1069.0    1000.0                           0  \n",
       "4    9000.0     689.0     679.0                           0  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = pd.read_csv(file_path)\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "213cf35e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21000, 25)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df,test_df = train_test_split(df1,test_size=0.3,random_state=42)\n",
    "preprocessing_obj = StandardScaler()\n",
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b44341f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "rfc_clf = RandomForestRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "605b5fab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor()"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_feature_train_df = train_df.drop(columns=['default'], axis=1)\n",
    "target_feature_train_df= train_df['default']\n",
    "rfc_clf.fit(input_feature_train_df,target_feature_train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e6a4d010",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8872864538015368"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = rfc_clf.predict(input_feature_train_df)\n",
    "r2_scores =r2_score(target_feature_train_df, y_pred)\n",
    "r2_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1bd82643",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21000, 24)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "input_feature_train_arr = preprocessing_obj.fit_transform(input_feature_train_df)\n",
    "input_feature_train_arr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f08a4e08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, ..., 0, 1, 1], dtype=int64)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(target_feature_train_df)\n",
    "# target_feature_train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fddd77a8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_arr = np.c_[input_feature_train_arr, np.array(target_feature_train_df)]\n",
    "# test_arr = np.c_[input_feature_test_arr, np.array[target_feature_test_df]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6b798727",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'keshav.npz'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_n = os.getcwd()\n",
    "path_csv = \"keshav.csv\"\n",
    "file_p = os.path.join(path_n, path_csv)\n",
    "\n",
    "os.path.basename(file_p).replace(\".csv\",\".npz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e8b75415",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.rename(mapper={'default.payment.next.month':\"default\"},axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0767204a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30000, 25)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ec107395",
   "metadata": {},
   "outputs": [],
   "source": [
    "file= \"UCI_Credit_Card.csv\"\n",
    "source_path = os.path.join(\"D:\\PycharmProjects\\DS_ML_Self\\MLops_classifcation\\notebooks\")\n",
    "source_file = os.path.join(source_path,file)\n",
    "\n",
    "a = \"D:\\\\PycharmProjects\\\\DS_ML_Self\\\\MLops_classifcation\\\\notebooks\\\\UCI_Credit_Card.csv\"\n",
    "\n",
    "excel = \"UCI_Credit_Card.csv\"\n",
    "dfz = pd.read_csv(a)\n",
    "target_path =os.path.join(\"D:\\\\PycharmProjects\\\\DS_ML_Self\\\\MLops_classifcation\\\\banking\\\\artifact\\\\data_ingestion\\\\2022-07-19-15-57-57\\\\tgz_data\")\n",
    "# \"\"D:\\PycharmProjects\\DS_ML_Self\\MLops_classifcation\\banking\\artifact\\data_ingestion\\2022-07-19-15-57-57\\tgz_data\"\"\n",
    "\n",
    "target_file = os.path.join(target_path,file)\n",
    "dfz.to_csv(\"D:\\\\PycharmProjects\\\\DS_ML_Self\\\\MLops_classifcation\\\\banking\\\\artifact\\\\data_ingestion\\\\2022-07-19-15-57-57\\\\tgz_data\\\\test_daata.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a552cf69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af7f10e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1977b652",
   "metadata": {},
   "source": [
    "# shutils for copying file form source to destination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b9993f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "shutil.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "833b7935",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\PycharmProjects\\\\DS_ML_Self\\\\MLops_classifcation\\\\banking\\\\artifact\\\\data_ingestion\\\\2022-07-19-15-57-57\\\\tgz_data\\\\UCI_Credit_Card.csv.zip'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src = \"D:\\\\PycharmProjects\\\\DS_ML_Self\\\\MLops_classifcation\\\\notebooks\\\\UCI_Credit_Card.csv.zip\"\n",
    "src_z = os.path.join(src)\n",
    "\n",
    "tg = \"D:\\\\PycharmProjects\\\\DS_ML_Self\\\\MLops_classifcation\\\\banking\\\\artifact\\\\data_ingestion\\\\2022-07-19-15-57-57\\\\tgz_data\"\n",
    "tg_z = os.path.join(tg)\n",
    "shutil.copy(src_z, tg_z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b1dccdba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ID', 'LIMIT_BAL', 'SEX', 'EDUCATION', 'MARRIAGE', 'AGE', 'PAY_0',\n",
       "       'PAY_2', 'PAY_3', 'PAY_4', 'PAY_5', 'PAY_6', 'BILL_AMT1', 'BILL_AMT2',\n",
       "       'BILL_AMT3', 'BILL_AMT4', 'BILL_AMT5', 'BILL_AMT6', 'PAY_AMT1',\n",
       "       'PAY_AMT2', 'PAY_AMT3', 'PAY_AMT4', 'PAY_AMT5', 'PAY_AMT6', 'default'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1e51074f",
   "metadata": {},
   "outputs": [],
   "source": [
    "AMOUNT_BILL = ['BILL_AMT1', 'BILL_AMT2', 'BILL_AMT3', 'BILL_AMT4', 'BILL_AMT5', 'BILL_AMT6',]\n",
    "AMOUTN_PRE_PAY = [ 'PAY_AMT1', 'PAY_AMT2', 'PAY_AMT3', 'PAY_AMT4', 'PAY_AMT5','PAY_AMT6',]\n",
    "REPAYMENT_STATUS = ['PAY_0', 'PAY_2', 'PAY_3', 'PAY_4', 'PAY_5', 'PAY_6']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ab955961",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df1['avg_amount_bill'] =df1['BILL_AMT1']+df1['BILL_AMT2']+df1['BILL_AMT3']+df1['BILL_AMT4']+df1['BILL_AMT5']+df1[BILL_AMT6']\n",
    "                                                                                                                 \n",
    "df1['avg_amount_bill'] = round((df1['BILL_AMT1']+df1['BILL_AMT2']+df1['BILL_AMT3']+df1['BILL_AMT4']+df1['BILL_AMT5']+df1['BILL_AMT6'])/6,4)                        \n",
    "df1['total_amount_bill'] = round((df1['BILL_AMT1']+df1['BILL_AMT2']+df1['BILL_AMT3']+df1['BILL_AMT4']+df1['BILL_AMT5']+df1['BILL_AMT6']),4)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c5c5195b",
   "metadata": {},
   "outputs": [],
   "source": [
    "'avg_amount_pay'\n",
    "'total_amount_pay'\n",
    "df1['avg_amount_pay'] = round((df1['PAY_AMT1']+df1['PAY_AMT2']+df1['PAY_AMT3']+df1['PAY_AMT4']+df1['PAY_AMT5']+df1['PAY_AMT6'])/6,4)\n",
    "\n",
    "df1['total_amount_pay'] = round((df1['PAY_AMT1']+df1['PAY_AMT2']+df1['PAY_AMT3']+df1['PAY_AMT4']+df1['PAY_AMT5']+df1['PAY_AMT6']),4)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ce812471",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['repayment_status'] = round((df1['PAY_0']+df1['PAY_2']+df1['PAY_3']+df1['PAY_4']+df1['PAY_5']+df1['PAY_6']),4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0d624c50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>LIMIT_BAL</th>\n",
       "      <th>SEX</th>\n",
       "      <th>EDUCATION</th>\n",
       "      <th>MARRIAGE</th>\n",
       "      <th>AGE</th>\n",
       "      <th>PAY_0</th>\n",
       "      <th>PAY_2</th>\n",
       "      <th>PAY_3</th>\n",
       "      <th>PAY_4</th>\n",
       "      <th>...</th>\n",
       "      <th>PAY_AMT3</th>\n",
       "      <th>PAY_AMT4</th>\n",
       "      <th>PAY_AMT5</th>\n",
       "      <th>PAY_AMT6</th>\n",
       "      <th>default</th>\n",
       "      <th>avg_amount_bill</th>\n",
       "      <th>total_amount_bill</th>\n",
       "      <th>avg_amount_pay</th>\n",
       "      <th>total_amount_pay</th>\n",
       "      <th>repayment_status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>20000.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1284.0000</td>\n",
       "      <td>7704.0</td>\n",
       "      <td>114.8333</td>\n",
       "      <td>689.0</td>\n",
       "      <td>-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>120000.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>26</td>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2846.1667</td>\n",
       "      <td>17077.0</td>\n",
       "      <td>833.3333</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>90000.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>16942.1667</td>\n",
       "      <td>101653.0</td>\n",
       "      <td>1836.3333</td>\n",
       "      <td>11018.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>50000.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>1100.0</td>\n",
       "      <td>1069.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>38555.6667</td>\n",
       "      <td>231334.0</td>\n",
       "      <td>1398.0000</td>\n",
       "      <td>8388.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>50000.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>57</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>9000.0</td>\n",
       "      <td>689.0</td>\n",
       "      <td>679.0</td>\n",
       "      <td>0</td>\n",
       "      <td>18223.1667</td>\n",
       "      <td>109339.0</td>\n",
       "      <td>9841.5000</td>\n",
       "      <td>59049.0</td>\n",
       "      <td>-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29995</th>\n",
       "      <td>29996</td>\n",
       "      <td>220000.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>39</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>5003.0</td>\n",
       "      <td>3047.0</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>120891.5000</td>\n",
       "      <td>725349.0</td>\n",
       "      <td>7091.6667</td>\n",
       "      <td>42550.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29996</th>\n",
       "      <td>29997</td>\n",
       "      <td>150000.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>43</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>...</td>\n",
       "      <td>8998.0</td>\n",
       "      <td>129.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3530.3333</td>\n",
       "      <td>21182.0</td>\n",
       "      <td>2415.0000</td>\n",
       "      <td>14490.0</td>\n",
       "      <td>-4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29997</th>\n",
       "      <td>29998</td>\n",
       "      <td>30000.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>37</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>...</td>\n",
       "      <td>22000.0</td>\n",
       "      <td>4200.0</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>3100.0</td>\n",
       "      <td>1</td>\n",
       "      <td>11749.3333</td>\n",
       "      <td>70496.0</td>\n",
       "      <td>5216.6667</td>\n",
       "      <td>31300.0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29998</th>\n",
       "      <td>29999</td>\n",
       "      <td>80000.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>41</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1178.0</td>\n",
       "      <td>1926.0</td>\n",
       "      <td>52964.0</td>\n",
       "      <td>1804.0</td>\n",
       "      <td>1</td>\n",
       "      <td>44435.1667</td>\n",
       "      <td>266611.0</td>\n",
       "      <td>24530.1667</td>\n",
       "      <td>147181.0</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29999</th>\n",
       "      <td>30000</td>\n",
       "      <td>50000.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>46</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1430.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1</td>\n",
       "      <td>38479.0000</td>\n",
       "      <td>230874.0</td>\n",
       "      <td>1384.6667</td>\n",
       "      <td>8308.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>30000 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          ID  LIMIT_BAL  SEX  EDUCATION  MARRIAGE  AGE  PAY_0  PAY_2  PAY_3  \\\n",
       "0          1    20000.0    2          2         1   24      2      2     -1   \n",
       "1          2   120000.0    2          2         2   26     -1      2      0   \n",
       "2          3    90000.0    2          2         2   34      0      0      0   \n",
       "3          4    50000.0    2          2         1   37      0      0      0   \n",
       "4          5    50000.0    1          2         1   57     -1      0     -1   \n",
       "...      ...        ...  ...        ...       ...  ...    ...    ...    ...   \n",
       "29995  29996   220000.0    1          3         1   39      0      0      0   \n",
       "29996  29997   150000.0    1          3         2   43     -1     -1     -1   \n",
       "29997  29998    30000.0    1          2         2   37      4      3      2   \n",
       "29998  29999    80000.0    1          3         1   41      1     -1      0   \n",
       "29999  30000    50000.0    1          2         1   46      0      0      0   \n",
       "\n",
       "       PAY_4  ...  PAY_AMT3  PAY_AMT4  PAY_AMT5  PAY_AMT6  default  \\\n",
       "0         -1  ...       0.0       0.0       0.0       0.0        1   \n",
       "1          0  ...    1000.0    1000.0       0.0    2000.0        1   \n",
       "2          0  ...    1000.0    1000.0    1000.0    5000.0        0   \n",
       "3          0  ...    1200.0    1100.0    1069.0    1000.0        0   \n",
       "4          0  ...   10000.0    9000.0     689.0     679.0        0   \n",
       "...      ...  ...       ...       ...       ...       ...      ...   \n",
       "29995      0  ...    5003.0    3047.0    5000.0    1000.0        0   \n",
       "29996     -1  ...    8998.0     129.0       0.0       0.0        0   \n",
       "29997     -1  ...   22000.0    4200.0    2000.0    3100.0        1   \n",
       "29998      0  ...    1178.0    1926.0   52964.0    1804.0        1   \n",
       "29999      0  ...    1430.0    1000.0    1000.0    1000.0        1   \n",
       "\n",
       "       avg_amount_bill  total_amount_bill  avg_amount_pay  total_amount_pay  \\\n",
       "0            1284.0000             7704.0        114.8333             689.0   \n",
       "1            2846.1667            17077.0        833.3333            5000.0   \n",
       "2           16942.1667           101653.0       1836.3333           11018.0   \n",
       "3           38555.6667           231334.0       1398.0000            8388.0   \n",
       "4           18223.1667           109339.0       9841.5000           59049.0   \n",
       "...                ...                ...             ...               ...   \n",
       "29995      120891.5000           725349.0       7091.6667           42550.0   \n",
       "29996        3530.3333            21182.0       2415.0000           14490.0   \n",
       "29997       11749.3333            70496.0       5216.6667           31300.0   \n",
       "29998       44435.1667           266611.0      24530.1667          147181.0   \n",
       "29999       38479.0000           230874.0       1384.6667            8308.0   \n",
       "\n",
       "       repayment_status  \n",
       "0                    -2  \n",
       "1                     3  \n",
       "2                     0  \n",
       "3                     0  \n",
       "4                    -2  \n",
       "...                 ...  \n",
       "29995                 0  \n",
       "29996                -4  \n",
       "29997                 8  \n",
       "29998                -1  \n",
       "29999                 0  \n",
       "\n",
       "[30000 rows x 30 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3b0b36a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 864x432 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "# sns.countplot(x='avg_amount_bill',hue='default',data=df1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0e78355d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BarContainer object of 30000 artists>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAOmklEQVR4nO3df4xlZ13H8ffHXcpvaMsOpO5u3SVZ0cW0UMYCQbFCkN2GuDHxjy7GYoVsGluD8Q9ZQoIa/gJiQgiFZUNWJCpFocKKC5UQERIs3am0pQtsO25rOy5xp6IY4Y+68PWPe2rv3t6Ze2e4++M8eb+Sk3vOc5577vdpZj9z5jn3nKaqkCT130+c7wIkSbNhoEtSIwx0SWqEgS5JjTDQJakRG8/XB2/atKm2bdt2vj5eknrprrvuerSq5sbtO2+Bvm3bNhYWFs7Xx0tSLyX515X2OeUiSY0w0CWpEQa6JDXCQJekRhjoktQIA12SGjEx0JMcSnIqyX0r7E+S9ydZTHJvkqtmX6YkaZJpztA/CuxaZf9uYEe37AM+9OOXJUlaq4mBXlVfBr67Spc9wMdq4A7g4iSXzapASdJ0ZjGHvhl4ZGh7qWt7kiT7kiwkWVheXl73B27b/3frfq8ktWoWgZ4xbWP/N0hVdbCq5qtqfm5u7KMIJEnrNItAXwK2Dm1vAU7O4LiSpDWYRaAfBq7vvu3yCuB7VfWdGRxXkrQGE5+2mOTjwDXApiRLwB8CTwGoqgPAEeBaYBH4AXDD2SpWkrSyiYFeVXsn7C/gpplVJElaF+8UlaRGGOiS1AgDXZIaYaBLUiMMdElqhIEuSY0w0CWpEQa6JDXCQJekRhjoktQIA12SGmGgS1IjDHRJaoSBLkmNMNAlqREGuiQ1wkCXpEYY6JLUCANdkhphoEtSIwx0SWqEgS5JjTDQJakRBrokNcJAl6RGGOiS1AgDXZIaYaBLUiMMdElqhIEuSY0w0CWpEQa6JDViqkBPsivJ8SSLSfaP2f/cJH+b5J4kx5LcMPtSJUmrmRjoSTYAtwC7gZ3A3iQ7R7rdBHyzqq4ErgH+JMlFM65VkrSKac7QrwYWq+pEVT0G3ArsGelTwLOTBHgW8F3g9EwrlSStappA3ww8MrS91LUN+wDws8BJ4BvAW6vqR6MHSrIvyUKSheXl5XWWLEkaZ5pAz5i2Gtl+PXA38JPAS4APJHnOk95UdbCq5qtqfm5ubo2lSpJWM02gLwFbh7a3MDgTH3YDcFsNLAIPAj8zmxIlSdOYJtCPAjuSbO8udF4HHB7p8zDwWoAkLwBeBJyYZaGSpNVtnNShqk4nuRm4HdgAHKqqY0lu7PYfAN4FfDTJNxhM0bytqh49i3VLkkZMDHSAqjoCHBlpOzC0fhL4ldmWJklaC+8UlaRGGOiS1AgDXZIaYaBLUiMMdElqhIEuSY0w0CWpEQa6JDXCQJekRhjoktQIA12SGmGgS1IjDHRJaoSBLkmNMNAlqREGuiQ1wkCXpEYY6JLUCANdkhphoEtSIwx0SWqEgS5JjTDQJakRBrokNcJAl6RGGOiS1AgDXZIaYaBLUiMMdElqhIEuSY0w0CWpEQa6JDViqkBPsivJ8SSLSfav0OeaJHcnOZbkH2dbpiRpko2TOiTZANwCvA5YAo4mOVxV3xzqczHwQWBXVT2c5PlnqV5J0gqmOUO/GlisqhNV9RhwK7BnpM8bgduq6mGAqjo12zIlSZNME+ibgUeGtpe6tmE/DVyS5EtJ7kpy/awKlCRNZ+KUC5AxbTXmOC8DXgs8HfinJHdU1f1nHCjZB+wDuPzyy9derSRpRdOcoS8BW4e2twAnx/T5fFV9v6oeBb4MXDl6oKo6WFXzVTU/Nze33polSWNME+hHgR1Jtie5CLgOODzS5zPALybZmOQZwMuBb822VEnSaiZOuVTV6SQ3A7cDG4BDVXUsyY3d/gNV9a0knwfuBX4EfKSq7jubhUuSzjTNHDpVdQQ4MtJ2YGT7vcB7Z1eaJGktvFNUkhphoEtSIwx0SWqEgS5JjTDQJakRBrokNcJAl6RGGOiS1AgDXZIaYaBLUiMMdElqhIEuSY0w0CWpEQa6JDXCQJekRhjoktQIA12SGmGgS1IjDHRJaoSBLkmNMNAlqREGuiQ1wkCXpEYY6JLUCANdkhphoEtSIwx0SWqEgS5JjTDQJakRBrokNcJAl6RGGOiS1AgDXZIaMVWgJ9mV5HiSxST7V+n380l+mOTXZ1eiJGkaEwM9yQbgFmA3sBPYm2TnCv3eDdw+6yIlSZNNc4Z+NbBYVSeq6jHgVmDPmH6/C3wKODXD+iRJU5om0DcDjwxtL3Vt/y/JZuDXgAOrHSjJviQLSRaWl5fXWqskaRXTBHrGtNXI9vuAt1XVD1c7UFUdrKr5qpqfm5ubskRJ0jQ2TtFnCdg6tL0FODnSZx64NQnAJuDaJKer6tOzKFKSNNk0gX4U2JFkO/BvwHXAG4c7VNX2x9eTfBT4rGEuSefWxECvqtNJbmbw7ZUNwKGqOpbkxm7/qvPmkqRzY5ozdKrqCHBkpG1skFfVb/34ZUmS1so7RSWpEQa6JDXCQJekRhjoktQIA12SGmGgS1IjDHRJaoSBLkmNMNAlqREGuiQ1wkCXpEYY6JLUCANdkhphoEtSIwx0SWqEgS5JjTDQJakRBrokNcJAl6RGGOiS1AgDXZIaYaBLUiMMdElqhIEuSY0w0CWpEQa6JDXCQJekRhjoktQIA12SGmGgS1IjDHRJaoSBLkmNmCrQk+xKcjzJYpL9Y/b/RpJ7u+WrSa6cfamSpNVMDPQkG4BbgN3ATmBvkp0j3R4EfqmqrgDeBRycdaGSpNVNc4Z+NbBYVSeq6jHgVmDPcIeq+mpV/We3eQewZbZlSpImmSbQNwOPDG0vdW0reTPwuXE7kuxLspBkYXl5efoqJUkTTRPoGdNWYzsmv8wg0N82bn9VHayq+aqan5ubm75KSdJEG6foswRsHdreApwc7ZTkCuAjwO6q+o/ZlCdJmtY0Z+hHgR1Jtie5CLgOODzcIcnlwG3Ab1bV/bMvU5I0ycQz9Ko6neRm4HZgA3Coqo4lubHbfwB4J/A84INJAE5X1fzZK1uSNGqaKReq6ghwZKTtwND6W4C3zLY0SdJaeKeoJDXCQJekRhjoktQIA12SGmGgS1IjDHRJaoSBLkmNMNAlqREGuiQ1wkCXpEYY6JLUCANdkhphoEtSIwx0SWqEgS5JjTDQJakRBrokNcJAl6RGGOiS1AgDXZIaYaBLUiMMdElqhIEuSY0w0CWpEQa6JDXCQJekRhjoktQIA12SGmGgS1IjDHRJaoSBLkmNMNAlqREGuiQ1YqpAT7IryfEki0n2j9mfJO/v9t+b5KrZlypJWs3EQE+yAbgF2A3sBPYm2TnSbTewo1v2AR+acZ2SpAmmOUO/GlisqhNV9RhwK7BnpM8e4GM1cAdwcZLLZlyrJGkVG6fosxl4ZGh7CXj5FH02A98Z7pRkH4MzeID/SXJ8TdU+YVPezaPrfO+FYhP0egx9rx/6P4a+1w+OYT1+aqUd0wR6xrTVOvpQVQeBg1N85uoFJQtVNf/jHud86vsY+l4/9H8Mfa8fHMOsTTPlsgRsHdreApxcRx9J0lk0TaAfBXYk2Z7kIuA64PBIn8PA9d23XV4BfK+qvjN6IEnS2TNxyqWqTie5Gbgd2AAcqqpjSW7s9h8AjgDXAovAD4Abzl7JwAymbS4AfR9D3+uH/o+h7/WDY5ipVD1pqluS1EPeKSpJjTDQJakRvQv0SY8hOAeffyjJqST3DbVdmuQLSR7oXi8Z2vf2rtbjSV4/1P6yJN/o9r0/Sbr2pyb5RNf+tSTbht7zpu4zHkjypnXWvzXJPyT5VpJjSd7awzE8LcmdSe7pxvDHfRtDd5wNSb6e5LM9rf+h7rPvTrLQtzEkuTjJJ5N8u/v38Mo+1T9WVfVmYXBR9l+AFwIXAfcAO89xDa8GrgLuG2p7D7C/W98PvLtb39nV+FRge1f7hm7fncArGXyH/3PA7q79d4AD3fp1wCe69UuBE93rJd36Jeuo/zLgqm792cD9XZ19GkOAZ3XrTwG+BryiT2PojvX7wF8Cn+3bz1F3rIeATSNtvRkD8GfAW7r1i4CL+1T/2DHN4iDnaun+o90+tP124O3noY5tnBnox4HLuvXLgOPj6mPwTaFXdn2+PdS+F/jwcJ9ufSODO9Ay3Kfb92Fg7wzG8hngdX0dA/AM4J8Z3L3cmzEwuFfji8BreCLQe1N/996HeHKg92IMwHOAB+m+GNK3+lda+jblstIjBs63F1T3vfvu9fld+0r1bu7WR9vPeE9VnQa+BzxvlWOtW/cn4EsZnOH2agzddMXdwCngC1XVtzG8D/gD4EdDbX2qHwZ3g/99krsyeKxHn8bwQmAZ+NNu2usjSZ7Zo/rH6lugT/WIgQvISvWuNo71vGfNkjwL+BTwe1X136t1XUc9Z30MVfXDqnoJgzPdq5P83CrdL6gxJHkDcKqq7pr2Leuo5Vz8HL2qqq5i8LTVm5K8epW+F9oYNjKYOv1QVb0U+D6DKZaVXGj1j9W3QL9QHzHw7+meLtm9nuraV6p3qVsfbT/jPUk2As8FvrvKsdYsyVMYhPlfVNVtfRzD46rqv4AvAbt6NIZXAb+a5CEGTy99TZI/71H9AFTVye71FPA3DJ7M2pcxLAFL3V92AJ9kEPB9qX+8WczbnKuFwW/VEwwuSjx+UfTF56GObZw5h/5ezryQ8p5u/cWceSHlBE9cSDnK4ELe4xdSru3ab+LMCyl/1a1fymDO75JueRC4dB21B/gY8L6R9j6NYQ64uFt/OvAV4A19GsPQWK7hiTn03tQPPBN49tD6Vxn8Uu3TGL4CvKhb/6Ou9t7UP3ZMszjIuVwYPGLgfgZXmd9xHj7/4wweC/y/DH7TvpnBvNgXgQe610uH+r+jq/U43dXvrn0euK/b9wGeuGv3acBfM3iMwp3AC4fe89td+yJwwzrr/wUGf97dC9zdLdf2bAxXAF/vxnAf8M6uvTdjGDrWNTwR6L2pn8Ec9D3dcozu32LPxvASYKH7Ofo0g3DtTf3jFm/9l6RG9G0OXZK0AgNdkhphoEtSIwx0SWqEgS5JjTDQJakRBrokNeL/APuge5iVh2y/AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plt.plot('avg_amount_bill','default',data=df1,kind='bar')\n",
    "plt.bar(x='avg_amount_pay',height='default',data=df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5f9a05b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "sns.countplot(x='total_amount_bill',hue='default',data=df1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0cc2a7aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 480x320 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(dpi=80)\n",
    "sns.countplot(x=\"EDUCATION\",hue=\"default\", data=df1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4fd0c1a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 864x432 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "# sns.countplot(x='default',hue='avg_amount_pay',data=df1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00bf7296",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "sns.countplot(x=\"total_amount_pay\",hue='default',data=df1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30458b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"total_amount_bill\" \"avg_amount_pay\" \"total_amount_pay\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "3b8d9fbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>housing_median_age</th>\n",
       "      <th>total_rooms</th>\n",
       "      <th>total_bedrooms</th>\n",
       "      <th>population</th>\n",
       "      <th>households</th>\n",
       "      <th>median_income</th>\n",
       "      <th>median_house_value</th>\n",
       "      <th>ocean_proximity</th>\n",
       "      <th>income_cat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-122.23</td>\n",
       "      <td>37.88</td>\n",
       "      <td>41.0</td>\n",
       "      <td>880.0</td>\n",
       "      <td>129.0</td>\n",
       "      <td>322.0</td>\n",
       "      <td>126.0</td>\n",
       "      <td>8.3252</td>\n",
       "      <td>452600.0</td>\n",
       "      <td>NEAR BAY</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   longitude  latitude  housing_median_age  total_rooms  total_bedrooms  \\\n",
       "0    -122.23     37.88                41.0        880.0           129.0   \n",
       "\n",
       "   population  households  median_income  median_house_value ocean_proximity  \\\n",
       "0       322.0       126.0         8.3252            452600.0        NEAR BAY   \n",
       "\n",
       "  income_cat  \n",
       "0          5  "
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import opendatasets as od\n",
    "import os\n",
    "dataset_url =\"https://www.kaggle.com/code/muskanbhasin/credit-card-default-prediction\"\n",
    "jsno_file = \"D:\\iNeuron\\internship_projects\"\n",
    "# js = os.path.join(jsno_file,\"kaggle.json\")\n",
    "params = {\"username\":\"keshavumaretiya\",\"key\":\"f9f56ba3bdbe907028e7e6698ea3decb\"}\n",
    "dfx = pd.read_csv(\"housing.csv\")\n",
    "dfx.head(1)\n",
    "dfx['median_income'].value_counts()\n",
    "dfx.shape\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "39839817",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        5\n",
       "1        5\n",
       "2        5\n",
       "3        4\n",
       "4        3\n",
       "        ..\n",
       "20635    2\n",
       "20636    2\n",
       "20637    2\n",
       "20638    2\n",
       "20639    2\n",
       "Name: income_cat, Length: 20640, dtype: category\n",
       "Categories (5, int64): [1 < 2 < 3 < 4 < 5]"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit,train_test_split\n",
    "split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "\n",
    "dfx = pd.read_csv(\"housing.csv\")\n",
    "\n",
    "dfx[\"income_cat\"] = pd.cut(\n",
    "    dfx[\"median_income\"],\n",
    "    bins=[0.0, 1.5, 3.0, 4.5, 6.0, np.inf],\n",
    "    labels=[1,2,3,4,5]\n",
    ")\n",
    "dfx.shape\n",
    "\n",
    "X = dfx.drop(\"median_house_value\", axis=1)\n",
    "y = dfx['median_house_value']\n",
    "# train_test_split(X,y, test_size=0.3, random_state=42)\n",
    "dfx.iloc[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "2fc5fce1",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The least populated class in y has only 1 member, which is too few. The minimum number of groups for any class cannot be less than 2.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [219]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m sss \u001b[38;5;241m=\u001b[39m StratifiedShuffleSplit(n_splits\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# sss.get_n_splits(X, y)\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m train_index, test_index \u001b[38;5;129;01min\u001b[39;00m sss\u001b[38;5;241m.\u001b[39msplit(X, y):\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTRAIN:\u001b[39m\u001b[38;5;124m\"\u001b[39m, train_index, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTEST:\u001b[39m\u001b[38;5;124m\"\u001b[39m, test_index)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:1600\u001b[0m, in \u001b[0;36mBaseShuffleSplit.split\u001b[1;34m(self, X, y, groups)\u001b[0m\n\u001b[0;32m   1570\u001b[0m \u001b[38;5;124;03m\"\"\"Generate indices to split data into training and test set.\u001b[39;00m\n\u001b[0;32m   1571\u001b[0m \n\u001b[0;32m   1572\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1597\u001b[0m \u001b[38;5;124;03mto an integer.\u001b[39;00m\n\u001b[0;32m   1598\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1599\u001b[0m X, y, groups \u001b[38;5;241m=\u001b[39m indexable(X, y, groups)\n\u001b[1;32m-> 1600\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m train, test \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iter_indices(X, y, groups):\n\u001b[0;32m   1601\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m train, test\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:1940\u001b[0m, in \u001b[0;36mStratifiedShuffleSplit._iter_indices\u001b[1;34m(self, X, y, groups)\u001b[0m\n\u001b[0;32m   1938\u001b[0m class_counts \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mbincount(y_indices)\n\u001b[0;32m   1939\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39mmin(class_counts) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m-> 1940\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1941\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe least populated class in y has only 1\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1942\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m member, which is too few. The minimum\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1943\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m number of groups for any class cannot\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1944\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m be less than 2.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1945\u001b[0m     )\n\u001b[0;32m   1947\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_train \u001b[38;5;241m<\u001b[39m n_classes:\n\u001b[0;32m   1948\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1949\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe train_size = \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m should be greater or \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1950\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mequal to the number of classes = \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (n_train, n_classes)\n\u001b[0;32m   1951\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: The least populated class in y has only 1 member, which is too few. The minimum number of groups for any class cannot be less than 2."
     ]
    }
   ],
   "source": [
    "# sss =StratifiedShuffleSplit(n_splits=1,test_size=0.2, random_state=42)\n",
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=0)\n",
    "# sss.get_n_splits(X, y)\n",
    "for train_index, test_index in sss.split(X, y):\n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "#     X_train, X_test = X.loc[train_index], X.loc[test_index]\n",
    "#     y_train, y_test = y.loc[train_index], y.loc[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "b764b1e8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The least populated class in y has only 1 member, which is too few. The minimum number of groups for any class cannot be less than 2.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [203]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# for i,j in split.split(X,y):\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#     print(X.iloc[i].drop(y,axis=1).shape)\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m#     print(X.iloc[j].drop(y,axis=1).shape)\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m X_train,X_test,y_train,y_test \u001b[38;5;241m=\u001b[39m split\u001b[38;5;241m.\u001b[39msplit(X,y)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:1600\u001b[0m, in \u001b[0;36mBaseShuffleSplit.split\u001b[1;34m(self, X, y, groups)\u001b[0m\n\u001b[0;32m   1570\u001b[0m \u001b[38;5;124;03m\"\"\"Generate indices to split data into training and test set.\u001b[39;00m\n\u001b[0;32m   1571\u001b[0m \n\u001b[0;32m   1572\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1597\u001b[0m \u001b[38;5;124;03mto an integer.\u001b[39;00m\n\u001b[0;32m   1598\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1599\u001b[0m X, y, groups \u001b[38;5;241m=\u001b[39m indexable(X, y, groups)\n\u001b[1;32m-> 1600\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m train, test \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iter_indices(X, y, groups):\n\u001b[0;32m   1601\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m train, test\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:1940\u001b[0m, in \u001b[0;36mStratifiedShuffleSplit._iter_indices\u001b[1;34m(self, X, y, groups)\u001b[0m\n\u001b[0;32m   1938\u001b[0m class_counts \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mbincount(y_indices)\n\u001b[0;32m   1939\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39mmin(class_counts) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m-> 1940\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1941\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe least populated class in y has only 1\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1942\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m member, which is too few. The minimum\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1943\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m number of groups for any class cannot\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1944\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m be less than 2.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1945\u001b[0m     )\n\u001b[0;32m   1947\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_train \u001b[38;5;241m<\u001b[39m n_classes:\n\u001b[0;32m   1948\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1949\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe train_size = \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m should be greater or \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1950\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mequal to the number of classes = \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (n_train, n_classes)\n\u001b[0;32m   1951\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: The least populated class in y has only 1 member, which is too few. The minimum number of groups for any class cannot be less than 2."
     ]
    }
   ],
   "source": [
    "# for i,j in split.split(X,y):\n",
    "#     print(X.iloc[i].drop(y,axis=1).shape)\n",
    "#     print(X.iloc[j].drop(y,axis=1).shape)\n",
    "X_train,X_test,y_train,y_test = split.split(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "73ae0796",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for train,test in split.split(dfx,dfx['median_income']):\n",
    "#     tr = dfx.loc[train].drop(['median_income'], axis=1)\n",
    "#     ts = dfx.loc[test].drop(['median_income'], axis=1)\n",
    "# print(tr.shape,ts.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "14cb8de3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\iNeuron\\\\internship_projects\\\\Credit Card Default Prediction'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# !pip install opendatasets\n",
    "# !pip install kaggle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "c94b3b3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.ipynb_checkpoints',\n",
       " 'default-of-credit-card-clients-dataset',\n",
       " 'housing.csv',\n",
       " 'notebooks.ipynb',\n",
       " 'practice.ipynb',\n",
       " 'Project Credit-default prediction 01.ipynb',\n",
       " 'testing',\n",
       " 'train',\n",
       " 'training.1600000.processed.noemoticon.csv.zip',\n",
       " 'UCI_Credit_Card.csv',\n",
       " 'Untitled.ipynb',\n",
       " 'xgb_banking.pkl',\n",
       " 'xgb_new.pkl']"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()\n",
    "os.listdir()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fee5308",
   "metadata": {},
   "source": [
    "## Downloading data from kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8c401be8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "import os\n",
    "api = KaggleApi()\n",
    "api.authenticate()\n",
    "\n",
    "link = \"https://www.kaggle.com/datasets/uciml/default-of-credit-card-clients-dataset?select=UCI_Credit_Card.csv\"\n",
    "os.getcwd()\n",
    "\n",
    "path1 = os.path.join(os.getcwd(),\"D:\\iNeuron\\internship_projects\\Credit Card Default Prediction\")\n",
    "\n",
    "api.dataset_download_file(\n",
    "    dataset='uciml/default-of-credit-card-clients-dataset',\n",
    "    file_name ='UCI_Credit_Card.csv',\n",
    "    path=path1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a9cc01",
   "metadata": {},
   "source": [
    "## unzipping a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d8ae9037",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\iNeuron\\internship_projects\\Credit Card Default Prediction\\UCI_Credit_Card.csv.zip\n",
      "D:\\PycharmProjects\\DS_ML_Self\\MLops_classifcation\\notebooks\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "file_zip =\"UCI_Credit_Card.csv.zip\"\n",
    "download_zip_path = os.path.join(path1,file_zip)\n",
    "print(download_zip_path)\n",
    "\n",
    "target_path = os.path.join(\"D:\\\\PycharmProjects\\\\DS_ML_Self\\\\MLops_classifcation\\\\notebooks\")\n",
    "print(target_path)\n",
    "with zipfile.ZipFile(download_zip_path, 'r') as zipref:\n",
    "    zipref.extractall(target_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44c66fdb",
   "metadata": {},
   "source": [
    "## TrainTest split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c13958ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "import pandas as pd, numpy as np\n",
    "split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3225b6a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['longitude',\n",
       " 'latitude',\n",
       " 'housing_median_age',\n",
       " 'total_rooms',\n",
       " 'total_bedrooms',\n",
       " 'population',\n",
       " 'households',\n",
       " 'median_income',\n",
       " 'median_house_value',\n",
       " 'ocean_proximity',\n",
       " 'income_cat']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = df['median_house_value']\n",
    "X = df.drop(labels='median_house_value',axis=1)\n",
    "y.shape,X.shape,df.shape\n",
    "df.columns.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ce228aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('housing.csv')\n",
    "df[\"income_cat\"] = pd.cut(df['median_income'], bins=[0.0, 1.5, 3.0, 4.5, 6.0,np.inf],labels=[1,2,3,4,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "200333b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for train_index,test_index in split.split(df, df[\"income_cat\"]):\n",
    "            strat_train_set = df.loc[train_index].drop([\"income_cat\"],axis=1)\n",
    "            strat_test_set = df.loc[test_index].drop([\"income_cat\"],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "7a2deca4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((16512, 10), (4128, 10))"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "strat_train_set.shape, strat_test_set.shape\n",
    "strat_test_set.columns\n",
    "split.split(df, df[\"income_cat\"])\n",
    "strat_train_set.shape, strat_test_set.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "75c0ceb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\PycharmProjects\\\\DS_ML_Self\\\\MLops_classifcation\\\\notebooks\\\\test'"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test_dir = os.makedirs(target_path, exist_ok=True)\n",
    "test_file_dir = \"test\"\n",
    "test_dirs_path = os.path.join(target_path,test_file_dir)\n",
    "\n",
    "os.makedirs(test_dirs_path,exist_ok = True)\n",
    "test_dirs_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af3ebee5",
   "metadata": {},
   "source": [
    "## creating dir and saving test.csv and train.csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "413d0f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_path()\n",
    "\"D:\\PycharmProjects\\DS_ML_Self\\MLops_classifcation\\notebooks\\testing\"\n",
    "path = os.path.join(\"D:\\\\PycharmProjects\\\\DS_ML_Self\\\\MLops_classifcation\\\\notebooks\\\\testing\")\n",
    "file_name = \"test.csv\"\n",
    "name_path = os.path.join(path,file_name)\n",
    "strat_test_set.to_csv(name_path,index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "45cff6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file_dir = \"train\"\n",
    "file_name_train = \"train.csv\"\n",
    "train_dirs_path = os.path.join(target_path,train_file_dir)\n",
    "os.makedirs(train_dirs_path, exist_ok = True)\n",
    "\n",
    "file_path_train_csv = os.path.join(train_dirs_path,file_name_train)\n",
    "strat_train_set.to_csv(file_path_train_csv,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c1d3bd1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\PycharmProjects\\\\DS_ML_Self\\\\MLops_classifcation\\\\notebooks'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af2a28d2",
   "metadata": {},
   "source": [
    "## coding for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1b7bfe1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\PycharmProjects\\\\DS_ML_Self\\\\MLops_classifcation\\\\notebooks'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ROOT_DIR = os.path.dirname(os.path.abspath(__file__))\n",
    "# CONFIG_PATH = os.path.join(ROOT_DIR, 'configuration.conf')\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5bf775bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "56175a9b",
   "metadata": {},
   "source": [
    "# Downloading kaggle dataset with authentication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "32b8181a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\PycharmProjects\\DS_ML_Self\\MLops_classifcation\\notebooks\\kaggle.json\n"
     ]
    }
   ],
   "source": [
    "import json \n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# your api key\n",
    "api_key = {\"username\":\"keshavumaretiya\",\"key\":\"f9f56ba3bdbe907028e7e6698ea3decb\"}\n",
    "\n",
    "# uses pathlib Path\n",
    "kaggle_path = Path('/root/.kaggle')\n",
    "os.makedirs(kaggle_path, exist_ok=True)\n",
    "\n",
    "# opens file and dumps python dict to json object \n",
    "# with open (kaggle_path/'kaggle.json', 'w') as handl:\n",
    "#     json.dump(api_key,handl)\n",
    "\n",
    "# os.chmod(kaggle_path/'kaggle.json', 600)  \n",
    "__file__ = 'kaggle.json'\n",
    "config = \"kaggle.json\"\n",
    "ROOT_DIR = os.path.dirname(os.path.abspath(__file__))\n",
    "CONFIG_PATH = os.path.join(ROOT_DIR, config)\n",
    "with open(CONFIG_PATH,'w') as kk:\n",
    "    json.dump(api_key,kk)\n",
    "print(CONFIG_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "15644230",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "import os\n",
    "import zipfile\n",
    "# os.environ[\"KAGGLE_CONFIG_DIR\"] = os.getcwd()\n",
    "# export KAGGLE_USERNAME=datadinosaur\n",
    "# export KAGGLE_KEY=xxxxxxxxxxxxxx\n",
    "# {\"username\":\"keshavumaretiya\",\"key\":\"f9f56ba3bdbe907028e7e6698ea3decb\"}\n",
    "\n",
    "api = KaggleApi()\n",
    "api.authenticate()\n",
    "download_url = \"https://www.kaggle.com/datasets/uciml/default-of-credit-card-clients-dataset?select=UCI_Credit_Card.csv\"\n",
    "dataset = \"uciml/default-of-credit-card-clients-dataset\"\n",
    "file_name = \"UCI_Credit_Card.csv\"\n",
    "\n",
    "ROOT_DIR = os.getcwd()\n",
    "dataset_dir = \"raw_data\"\n",
    "dataset_path = os.path.join(ROOT_DIR,dataset_dir)\n",
    "os.makedirs(dataset_path,exist_ok = True)\n",
    "\n",
    "uci_zip_data = \"zip_data\"\n",
    "zip_data_dir =os.path.join(ROOT_DIR,uci_zip_data)\n",
    "os.makedirs(zip_data_dir,exist_ok = True)\n",
    "\n",
    "download_url = \"https://www.kaggle.com/datasets/uciml/default-of-credit-card-clients-dataset?select=UCI_Credit_Card.csv\"\n",
    "dataset = \"uciml/default-of-credit-card-clients-dataset\"\n",
    "file_name = \"UCI_Credit_Card.csv\"\n",
    "\n",
    "def download_bankig_data():\n",
    "\n",
    "    uci_zip_data = \"zip_data\"\n",
    "    zip_data_dir =os.path.join(ROOT_DIR,uci_zip_data)\n",
    "    os.makedirs(zip_data_dir,exist_ok = True)\n",
    "    \n",
    "    api.dataset_download_file(\n",
    "        dataset='uciml/default-of-credit-card-clients-dataset',\n",
    "        file_name ='UCI_Credit_Card.csv',\n",
    "        path=zip_data_dir)\n",
    "    \n",
    "    zipped_file_name = \"UCI_Credit_Card.csv.zip\"\n",
    "    zip_file_path = os.path.join(zip_data_dir,zipped_file_name)\n",
    "    return zip_file_path\n",
    "\n",
    "        \n",
    "def exctracted_tgz_file(zip_file_path):\n",
    "    raw_data_dir = dataset_path\n",
    "    os.makedirs(raw_data_dir, exist_ok=True)\n",
    "    \n",
    "    with zipfile.ZipFile(zip_file_path, 'r') as zipref:\n",
    "        zipref.extractall(raw_data_dir)\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "8269538c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\PycharmProjects\\DS_ML_Self\\MLops_classifcation\\notebooks\\raw_data\n"
     ]
    }
   ],
   "source": [
    "# print(download_bankig_data())\n",
    "print(dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "4fb02311",
   "metadata": {},
   "outputs": [],
   "source": [
    "zip_files = download_bankig_data()\n",
    "exctracted_tgz_file(zip_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e528124d",
   "metadata": {},
   "source": [
    "## learnings only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "895564e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\PycharmProjects\\\\DS_ML_Self\\\\MLops_classifcation\\\\notebooks'"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "report_file_path = train_dirs_path\n",
    "os.path.dirname(report_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "88c34950",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'13a5565b-9033-444c-a148-e7d3e4f5960f'"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from threading import Thread\n",
    "import uuid\n",
    "str(uuid.uuid4())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "c494e010",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Experiment(experiment_id=None, initialization_timestamp=None, artifact_time_stamp=None, running_status=None, start_time=None, stop_time=None, execution_time=None, message=None, experiment_file_path=None, accuracy=None, is_model_accepted=None)"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import namedtuple\n",
    "Experiment = namedtuple(\"Experiment\", [\"experiment_id\", \"initialization_timestamp\", \"artifact_time_stamp\",\n",
    "                                       \"running_status\", \"start_time\", \"stop_time\", \"execution_time\", \"message\",\n",
    "                                       \"experiment_file_path\", \"accuracy\", \"is_model_accepted\"])\n",
    "Experiment(* ([None] * 11))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc5f846",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e3db5d04",
   "metadata": {},
   "source": [
    "# coding strategy :- \n",
    "## 1. Data ingestion only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd46c440",
   "metadata": {},
   "outputs": [],
   "source": [
    "# yaml file -->dir: config\n",
    "\n",
    "training_pipeline_config:\n",
    "  pipeline_name: housing\n",
    "  artifact_dir: artifact\n",
    "\n",
    "\n",
    "data_ingestion_config:\n",
    "  dataset_download_url: https://raw.githubusercontent.com/ageron/handson-ml/master/datasets/housing/housing.tgz\n",
    "  raw_data_dir: raw_data\n",
    "  tgz_download_dir: tgz_data\n",
    "  ingested_dir: ingested_data\n",
    "  ingested_train_dir: train\n",
    "  ingested_test_dir: test \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ec23d448",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\PycharmProjects\\\\DS_ML_Self\\\\MLops_classifcation\\\\notebooks\\\\config\\\\config.yaml'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ROOT_DIR = os.getcwd()\n",
    "CONFIG_DIR = 'config'\n",
    "CONFIG_FILE_NAME = 'config.yaml'\n",
    "CONFIG_FILE_PATH= os.path.join(ROOT_DIR,CONFIG_DIR,CONFIG_FILE_NAME)\n",
    "CONFIG_FILE_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f740027",
   "metadata": {},
   "outputs": [],
   "source": [
    "# constant:dir : Constant file -->dir: housing\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "def get_current_time_stamp():\n",
    "    return f\"{datetime.now().strftime('%Y-%m-%d-%H-%M-%S')}\"\n",
    "  \n",
    "ROOT_DIR = os.getcwd()  #to get current working directory\n",
    "CONFIG_DIR = \"config\"\n",
    "CONFIG_FILE_NAME = \"config.yaml\"\n",
    "CONFIG_FILE_PATH = os.path.join(ROOT_DIR,CONFIG_DIR,CONFIG_FILE_NAME)\n",
    "\n",
    "CURRENT_TIME_STAMP = get_current_time_stamp()\n",
    "\n",
    "# Training pipeline related variable\n",
    "TRAINING_PIPELINE_CONFIG_KEY = \"training_pipeline_config\"\n",
    "TRAINING_PIPELINE_ARTIFACT_DIR_KEY = \"artifact_dir\"\n",
    "TRAINING_PIPELINE_NAME_KEY = \"pipeline_name\"\n",
    "\n",
    "DATA_INGESTION_ARTIFACT_DIR = \"data_ingestion\"\n",
    "DATA_INGESTION_CONFIG_KEY = \"data_ingestion_config\"\n",
    "DATA_INGESTION_DOWNLOAD_URL_KEY = \"dataset_download_url\"\n",
    "DATA_INGESTION_RAW_DATA_DIR_KEY = \"raw_data_dir\"\n",
    "DATA_INGESTION_TGZ_DOWNLOAD_DIR_KEY = \"tgz_download_dir\"\n",
    "DATA_INGESTION_INGESTED_DIR_NAME_KEY = \"ingested_dir\"\n",
    "DATA_INGESTION_TRAIN_DIR_KEY = \"ingested_train_dir\"\n",
    "DATA_INGESTION_TEST_DIR_KEY = \"ingested_test_dir\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e26ccd7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# artifact_entity.py file  -->housing/entity dir\n",
    "import namedtuple\n",
    "\n",
    "DataIngestionArtifact = namedtuple(\"DataIngestionArtifact\",\n",
    "[ \"train_file_path\", \"test_file_path\", \"is_ingested\", \"message\"])\n",
    "\n",
    "# config_entity.py file  -->housing/entity dir\n",
    "DataIngestionConfig=namedtuple(\"DataIngestionConfig\",\n",
    "[\"dataset_download_url\",\"tgz_download_dir\",\"raw_data_dir\",\"ingested_train_dir\",\"ingested_test_dir\"])\n",
    "\n",
    "\n",
    "# config_entity.py file  -->housing/entity dir\n",
    "TrainingPipelineConfig = namedtuple(\"TrainingPipelineConfig\", [\"artifact_dir\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb50746",
   "metadata": {},
   "outputs": [],
   "source": [
    "# util.py file  -->housing/util dir\n",
    "import yaml,os\n",
    "\n",
    "def read_yaml_file(file_path:str)->dict:\n",
    "    \"\"\"\n",
    "    Reads a YAML file and returns the contents as a dictionary.\n",
    "    file_path: str\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'rb') as yaml_file:\n",
    "            return yaml.safe_load(yaml_file)\n",
    "    except Exception as e:\n",
    "        raise HousingException(e,sys) from e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0de004c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# configuration :file  --> housing/config dir\n",
    "from housing.entity.config_entity import DataIngestionConfig, DataTransformationConfig,DataValidationConfig,   \\\n",
    "ModelTrainerConfig,ModelEvaluationConfig,ModelPusherConfig,TrainingPipelineConfig\n",
    "from housing.util.util import read_yaml_file\n",
    "from housing.logger import logging\n",
    "import sys,os\n",
    "from housing.constant import *\n",
    "from housing.exception import HousingException\n",
    "\n",
    "\n",
    "class Configuartion:\n",
    "\n",
    "    def __init__(self,\n",
    "        config_file_path:str =CONFIG_FILE_PATH,\n",
    "        current_time_stamp:str = CURRENT_TIME_STAMP\n",
    "        ) -> None:\n",
    "        try:\n",
    "            self.config_info  = read_yaml_file(file_path=config_file_path)\n",
    "            self.training_pipeline_config = self.get_training_pipeline_config()\n",
    "            self.time_stamp = current_time_stamp\n",
    "        except Exception as e:\n",
    "            raise HousingException(e,sys) from e\n",
    "\n",
    "\n",
    "    def get_data_ingestion_config(self) ->DataIngestionConfig:\n",
    "        try:\n",
    "            artifact_dir = self.training_pipeline_config.artifact_dir\n",
    "            data_ingestion_artifact_dir=os.path.join(\n",
    "                artifact_dir,\n",
    "                DATA_INGESTION_ARTIFACT_DIR,\n",
    "                self.time_stamp\n",
    "            )\n",
    "            data_ingestion_info = self.config_info[DATA_INGESTION_CONFIG_KEY]\n",
    "            \n",
    "            dataset_download_url = data_ingestion_info[DATA_INGESTION_DOWNLOAD_URL_KEY]\n",
    "            tgz_download_dir = os.path.join(\n",
    "                data_ingestion_artifact_dir,\n",
    "                data_ingestion_info[DATA_INGESTION_TGZ_DOWNLOAD_DIR_KEY]\n",
    "            )\n",
    "            raw_data_dir = os.path.join(data_ingestion_artifact_dir,\n",
    "            data_ingestion_info[DATA_INGESTION_RAW_DATA_DIR_KEY]\n",
    "            )\n",
    "\n",
    "            ingested_data_dir = os.path.join(\n",
    "                data_ingestion_artifact_dir,\n",
    "                data_ingestion_info[DATA_INGESTION_INGESTED_DIR_NAME_KEY]\n",
    "            )\n",
    "            ingested_train_dir = os.path.join(\n",
    "                ingested_data_dir,\n",
    "                data_ingestion_info[DATA_INGESTION_TRAIN_DIR_KEY]\n",
    "            )\n",
    "            ingested_test_dir =os.path.join(\n",
    "                ingested_data_dir,\n",
    "                data_ingestion_info[DATA_INGESTION_TEST_DIR_KEY]\n",
    "            )\n",
    "\n",
    "\n",
    "            data_ingestion_config=DataIngestionConfig(\n",
    "                dataset_download_url=dataset_download_url, \n",
    "                tgz_download_dir=tgz_download_dir, \n",
    "                raw_data_dir=raw_data_dir, \n",
    "                ingested_train_dir=ingested_train_dir, \n",
    "                ingested_test_dir=ingested_test_dir\n",
    "            )\n",
    "            logging.info(f\"Data Ingestion config: {data_ingestion_config}\")\n",
    "            return data_ingestion_config\n",
    "        except Exception as e:\n",
    "            raise HousingException(e,sys) from e\n",
    "            \n",
    "    #### -------###\n",
    "    \n",
    "    def get_training_pipeline_config(self) ->TrainingPipelineConfig:\n",
    "        try:\n",
    "            training_pipeline_config = self.config_info[TRAINING_PIPELINE_CONFIG_KEY]\n",
    "            artifact_dir = os.path.join(ROOT_DIR,\n",
    "            training_pipeline_config[TRAINING_PIPELINE_NAME_KEY],\n",
    "            training_pipeline_config[TRAINING_PIPELINE_ARTIFACT_DIR_KEY]\n",
    "            )\n",
    "\n",
    "            training_pipeline_config = TrainingPipelineConfig(artifact_dir=artifact_dir)\n",
    "            logging.info(f\"Training pipleine config: {training_pipeline_config}\")\n",
    "            return training_pipeline_config\n",
    "        except Exception as e:\n",
    "            raise HousingException(e,sys) from e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44233758",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_ingestion.py -->housing/component dir\n",
    "\n",
    "from housing.entity.config_entity import DataIngestionConfig\n",
    "from housing.entity.artifact_entity import DataIngestionArtifact\n",
    "import sys,os\n",
    "from housing.exception import HousingException\n",
    "from housing.logger import logging\n",
    "import tarfile\n",
    "import numpy as np\n",
    "from six.moves import urllib\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "class DataIngestion:\n",
    "\n",
    "    def __init__(self,data_ingestion_config:DataIngestionConfig ):\n",
    "        try:\n",
    "            logging.info(f\"{'>>'*20}Data Ingestion log started.{'<<'*20} \")\n",
    "            self.data_ingestion_config = data_ingestion_config\n",
    "\n",
    "        except Exception as e:\n",
    "            raise HousingException(e,sys)\n",
    "    \n",
    "\n",
    "    def download_housing_data(self,) -> str:\n",
    "        try:\n",
    "            #extraction remote url to download dataset\n",
    "            download_url = self.data_ingestion_config.dataset_download_url\n",
    "\n",
    "            #folder location to download file\n",
    "            tgz_download_dir = self.data_ingestion_config.tgz_download_dir\n",
    "            \n",
    "            os.makedirs(tgz_download_dir,exist_ok=True)\n",
    "\n",
    "            housing_file_name = os.path.basename(download_url)\n",
    "\n",
    "            tgz_file_path = os.path.join(tgz_download_dir, housing_file_name)\n",
    "\n",
    "            logging.info(f\"Downloading file from :[{download_url}] into :[{tgz_file_path}]\")\n",
    "            urllib.request.urlretrieve(download_url, tgz_file_path)\n",
    "            logging.info(f\"File :[{tgz_file_path}] has been downloaded successfully.\")\n",
    "            return tgz_file_path\n",
    "\n",
    "        except Exception as e:\n",
    "            raise HousingException(e,sys) from e\n",
    "\n",
    "    def extract_tgz_file(self,tgz_file_path:str):\n",
    "        try:\n",
    "            raw_data_dir = self.data_ingestion_config.raw_data_dir\n",
    "\n",
    "            if os.path.exists(raw_data_dir):\n",
    "                os.remove(raw_data_dir)\n",
    "\n",
    "            os.makedirs(raw_data_dir,exist_ok=True)\n",
    "\n",
    "            logging.info(f\"Extracting tgz file: [{tgz_file_path}] into dir: [{raw_data_dir}]\")\n",
    "            with tarfile.open(tgz_file_path) as housing_tgz_file_obj:\n",
    "                housing_tgz_file_obj.extractall(path=raw_data_dir)\n",
    "            logging.info(f\"Extraction completed\")\n",
    "\n",
    "        except Exception as e:\n",
    "            raise HousingException(e,sys) from e\n",
    "    \n",
    "    def split_data_as_train_test(self) -> DataIngestionArtifact:\n",
    "        try:\n",
    "            raw_data_dir = self.data_ingestion_config.raw_data_dir\n",
    "\n",
    "            file_name = os.listdir(raw_data_dir)[0]\n",
    "\n",
    "            housing_file_path = os.path.join(raw_data_dir,file_name)\n",
    "\n",
    "\n",
    "            logging.info(f\"Reading csv file: [{housing_file_path}]\")\n",
    "            housing_data_frame = pd.read_csv(housing_file_path)\n",
    "\n",
    "            housing_data_frame[\"income_cat\"] = pd.cut(\n",
    "                housing_data_frame[\"median_income\"],\n",
    "                bins=[0.0, 1.5, 3.0, 4.5, 6.0, np.inf],\n",
    "                labels=[1,2,3,4,5]\n",
    "            )\n",
    "            \n",
    "\n",
    "            logging.info(f\"Splitting data into train and test\")\n",
    "            strat_train_set = None\n",
    "            strat_test_set = None\n",
    "\n",
    "            split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "\n",
    "            for train_index,test_index in split.split(housing_data_frame, housing_data_frame[\"income_cat\"]):\n",
    "                strat_train_set = housing_data_frame.loc[train_index].drop([\"income_cat\"],axis=1)\n",
    "                strat_test_set = housing_data_frame.loc[test_index].drop([\"income_cat\"],axis=1)\n",
    "\n",
    "            train_file_path = os.path.join(self.data_ingestion_config.ingested_train_dir,\n",
    "                                            file_name)\n",
    "\n",
    "            test_file_path = os.path.join(self.data_ingestion_config.ingested_test_dir,\n",
    "                                        file_name)\n",
    "            \n",
    "            if strat_train_set is not None:\n",
    "                os.makedirs(self.data_ingestion_config.ingested_train_dir,exist_ok=True)\n",
    "                logging.info(f\"Exporting training datset to file: [{train_file_path}]\")\n",
    "                strat_train_set.to_csv(train_file_path,index=False)\n",
    "\n",
    "            if strat_test_set is not None:\n",
    "                os.makedirs(self.data_ingestion_config.ingested_test_dir, exist_ok= True)\n",
    "                logging.info(f\"Exporting test dataset to file: [{test_file_path}]\")\n",
    "                strat_test_set.to_csv(test_file_path,index=False)\n",
    "            \n",
    "\n",
    "            data_ingestion_artifact = DataIngestionArtifact(train_file_path=train_file_path,\n",
    "                                test_file_path=test_file_path,\n",
    "                                is_ingested=True,\n",
    "                                message=f\"Data ingestion completed successfully.\"\n",
    "                                )\n",
    "            logging.info(f\"Data Ingestion artifact:[{data_ingestion_artifact}]\")\n",
    "            return data_ingestion_artifact\n",
    "\n",
    "        except Exception as e:\n",
    "            raise HousingException(e,sys) from e\n",
    "\n",
    "    def initiate_data_ingestion(self)-> DataIngestionArtifact:\n",
    "        try:\n",
    "            tgz_file_path =  self.download_housing_data()\n",
    "            self.extract_tgz_file(tgz_file_path=tgz_file_path)\n",
    "            return self.split_data_as_train_test()\n",
    "        except Exception as e:\n",
    "            raise HousingException(e,sys) from e\n",
    "    \n",
    "\n",
    "\n",
    "    def __del__(self):\n",
    "        logging.info(f\"{'>>'*20}Data Ingestion log completed.{'<<'*20} \\n\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f071116",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline.py file --->housing/pipline dir\n",
    "\n",
    "class Pipeline(Thread):\n",
    "    experiment: Experiment = Experiment(*([None] * 11))\n",
    "    experiment_file_path = None\n",
    "\n",
    "    def __init__(self, config: Configuartion ) -> None:\n",
    "        try:\n",
    "            os.makedirs(config.training_pipeline_config.artifact_dir, exist_ok=True)\n",
    "            Pipeline.experiment_file_path=os.path.join(config.training_pipeline_config.artifact_dir,EXPERIMENT_DIR_NAME, EXPERIMENT_FILE_NAME)\n",
    "            super().__init__(daemon=False, name=\"pipeline\")\n",
    "            self.config = config\n",
    "        except Exception as e:\n",
    "            raise HousingException(e, sys) from e\n",
    "\n",
    "    def start_data_ingestion(self) -> DataIngestionArtifact:\n",
    "        try:\n",
    "            data_ingestion = DataIngestion(data_ingestion_config=self.config.get_data_ingestion_config())\n",
    "            return data_ingestion.initiate_data_ingestion()\n",
    "        except Exception as e:\n",
    "            raise HousingException(e, sys) from e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4877a588",
   "metadata": {},
   "source": [
    "## 2. DataValidation only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e6017f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# config.yaml file \n",
    "\n",
    "data_validation_config:\n",
    "  schema_dir: config\n",
    "  schema_file_name: schema.yaml\n",
    "  report_file_name: report.json\n",
    "  report_page_file_name: report.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7d7838",
   "metadata": {},
   "outputs": [],
   "source": [
    "# constant dir\n",
    "\n",
    "# Data Validation related variables\n",
    "DATA_VALIDATION_CONFIG_KEY = \"data_validation_config\"\n",
    "DATA_VALIDATION_SCHEMA_FILE_NAME_KEY = \"schema_file_name\"\n",
    "DATA_VALIDATION_SCHEMA_DIR_KEY = \"schema_dir\"\n",
    "DATA_VALIDATION_ARTIFACT_DIR_NAME=\"data_validation\"\n",
    "DATA_VALIDATION_REPORT_FILE_NAME_KEY = \"report_file_name\"\n",
    "DATA_VALIDATION_REPORT_PAGE_FILE_NAME_KEY = \"report_page_file_name\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a0d6535e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# artifact_enitity.file --banking/entity dir\n",
    "from collections import namedtuple\n",
    "DataValidationArtifact = namedtuple(\"DataValidationArtifact\",[\"schema_file_path\",\"report_file_path\",\"report_page_file_path\",\"is_validated\",\"message\"])\n",
    "\n",
    "# config_enitity.file --banking/entity dir\n",
    "DataValidationConfig = namedtuple(\"DataValidationConfig\", [\"schema_file_path\",\"report_file_path\",\"report_page_file_path\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd1cdaa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# configuration.py file --> bankig/config dir\n",
    "\n",
    "    def get_data_validation_config(self) -> DataValidationConfig:\n",
    "        try:\n",
    "            artifact_dir = self.training_pipeline_config.artifact_dir\n",
    "\n",
    "            data_validation_artifact_dir=os.path.join(\n",
    "                artifact_dir,\n",
    "                DATA_VALIDATION_ARTIFACT_DIR_NAME,\n",
    "                self.time_stamp\n",
    "            )\n",
    "            data_validation_config = self.config_info[DATA_VALIDATION_CONFIG_KEY]\n",
    "\n",
    "\n",
    "            schema_file_path = os.path.join(ROOT_DIR,\n",
    "            data_validation_config[DATA_VALIDATION_SCHEMA_DIR_KEY],\n",
    "            data_validation_config[DATA_VALIDATION_SCHEMA_FILE_NAME_KEY]\n",
    "            )\n",
    "\n",
    "            report_file_path = os.path.join(data_validation_artifact_dir,\n",
    "            data_validation_config[DATA_VALIDATION_REPORT_FILE_NAME_KEY]\n",
    "            )\n",
    "\n",
    "            report_page_file_path = os.path.join(data_validation_artifact_dir,\n",
    "            data_validation_config[DATA_VALIDATION_REPORT_PAGE_FILE_NAME_KEY]\n",
    "\n",
    "            )\n",
    "\n",
    "            data_validation_config = DataValidationConfig(\n",
    "                schema_file_path=schema_file_path,\n",
    "                report_file_path=report_file_path,\n",
    "                report_page_file_path=report_page_file_path,\n",
    "            )\n",
    "            return data_validation_config\n",
    "        except Exception as e:\n",
    "            raise HousingException(e,sys) from e\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17cb0299",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_validation.py file --> banking/component dir\n",
    "\n",
    "\n",
    "from housing.logger import logging\n",
    "from housing.exception import HousingException\n",
    "from housing.entity.config_entity import DataValidationConfig\n",
    "from housing.entity.artifact_entity import DataIngestionArtifact,DataValidationArtifact\n",
    "import os,sys\n",
    "import pandas  as pd\n",
    "from evidently.model_profile import Profile\n",
    "from evidently.model_profile.sections import DataDriftProfileSection\n",
    "from evidently.dashboard import Dashboard\n",
    "from evidently.dashboard.tabs import DataDriftTab\n",
    "import json\n",
    "\n",
    "class DataValidation:\n",
    "    \n",
    "\n",
    "    def __init__(self, data_validation_config:DataValidationConfig,\n",
    "        data_ingestion_artifact:DataIngestionArtifact):\n",
    "        try:\n",
    "            logging.info(f\"{'>>'*30}Data Valdaition log started.{'<<'*30} \\n\\n\")\n",
    "            self.data_validation_config = data_validation_config\n",
    "            self.data_ingestion_artifact = data_ingestion_artifact\n",
    "        except Exception as e:\n",
    "            raise HousingException(e,sys) from e\n",
    "\n",
    "\n",
    "    def get_train_and_test_df(self):\n",
    "        try:\n",
    "            train_df = pd.read_csv(self.data_ingestion_artifact.train_file_path)\n",
    "            test_df = pd.read_csv(self.data_ingestion_artifact.test_file_path)\n",
    "            return train_df,test_df\n",
    "        except Exception as e:\n",
    "            raise HousingException(e,sys) from e\n",
    "\n",
    "\n",
    "    def is_train_test_file_exists(self)->bool:\n",
    "        try:\n",
    "            logging.info(\"Checking if training and test file is available\")\n",
    "            is_train_file_exist = False\n",
    "            is_test_file_exist = False\n",
    "\n",
    "            train_file_path = self.data_ingestion_artifact.train_file_path\n",
    "            test_file_path = self.data_ingestion_artifact.test_file_path\n",
    "\n",
    "            is_train_file_exist = os.path.exists(train_file_path)\n",
    "            is_test_file_exist = os.path.exists(test_file_path)\n",
    "\n",
    "            is_available =  is_train_file_exist and is_test_file_exist\n",
    "\n",
    "            logging.info(f\"Is train and test file exists?-> {is_available}\")\n",
    "            \n",
    "            if not is_available:\n",
    "                training_file = self.data_ingestion_artifact.train_file_path\n",
    "                testing_file = self.data_ingestion_artifact.test_file_path\n",
    "                message=f\"Training file: {training_file} or Testing file: {testing_file}\" \\\n",
    "                    \"is not present\"\n",
    "                raise Exception(message)\n",
    "\n",
    "            return is_available\n",
    "        except Exception as e:\n",
    "            raise HousingException(e,sys) from e\n",
    "\n",
    "    \n",
    "    def validate_dataset_schema(self)->bool:\n",
    "        try:\n",
    "            validation_status = False\n",
    "            \n",
    "            #Assigment validate training and testing dataset using schema file\n",
    "            #1. Number of Column\n",
    "            #2. Check the value of ocean proximity \n",
    "            # acceptable values     <1H OCEAN\n",
    "            # INLAND\n",
    "            # ISLAND\n",
    "            # NEAR BAY\n",
    "            # NEAR OCEAN\n",
    "            #3. Check column names\n",
    "\n",
    "\n",
    "            validation_status = True\n",
    "            return validation_status \n",
    "        except Exception as e:\n",
    "            raise HousingException(e,sys) from e\n",
    "\n",
    "    def get_and_save_data_drift_report(self):\n",
    "        try:\n",
    "            profile = Profile(sections=[DataDriftProfileSection()])\n",
    "\n",
    "            train_df,test_df = self.get_train_and_test_df()\n",
    "\n",
    "            profile.calculate(train_df,test_df)\n",
    "\n",
    "            report = json.loads(profile.json())\n",
    "\n",
    "            report_file_path = self.data_validation_config.report_file_path\n",
    "            report_dir = os.path.dirname(report_file_path)\n",
    "            os.makedirs(report_dir,exist_ok=True)\n",
    "\n",
    "            with open(report_file_path,\"w\") as report_file:\n",
    "                json.dump(report, report_file, indent=6)\n",
    "            return report\n",
    "        except Exception as e:\n",
    "            raise HousingException(e,sys) from e\n",
    "\n",
    "    def save_data_drift_report_page(self):\n",
    "        try:\n",
    "            dashboard = Dashboard(tabs=[DataDriftTab()])\n",
    "            train_df,test_df = self.get_train_and_test_df()\n",
    "            dashboard.calculate(train_df,test_df)\n",
    "\n",
    "            report_page_file_path = self.data_validation_config.report_page_file_path\n",
    "            report_page_dir = os.path.dirname(report_page_file_path)\n",
    "            os.makedirs(report_page_dir,exist_ok=True)\n",
    "\n",
    "            dashboard.save(report_page_file_path)\n",
    "        except Exception as e:\n",
    "            raise HousingException(e,sys) from e\n",
    "\n",
    "    def is_data_drift_found(self)->bool:\n",
    "        try:\n",
    "            report = self.get_and_save_data_drift_report()\n",
    "            self.save_data_drift_report_page()\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            raise HousingException(e,sys) from e\n",
    "\n",
    "    def initiate_data_validation(self)->DataValidationArtifact :\n",
    "        try:\n",
    "            self.is_train_test_file_exists()\n",
    "            self.validate_dataset_schema()\n",
    "            self.is_data_drift_found()\n",
    "\n",
    "            data_validation_artifact = DataValidationArtifact(\n",
    "                schema_file_path=self.data_validation_config.schema_file_path,\n",
    "                report_file_path=self.data_validation_config.report_file_path,\n",
    "                report_page_file_path=self.data_validation_config.report_page_file_path,\n",
    "                is_validated=True,\n",
    "                message=\"Data Validation performed successully.\"\n",
    "            )\n",
    "            logging.info(f\"Data validation artifact: {data_validation_artifact}\")\n",
    "            return data_validation_artifact\n",
    "        except Exception as e:\n",
    "            raise HousingException(e,sys) from e\n",
    "\n",
    "\n",
    "    def __del__(self):\n",
    "        logging.info(f\"{'>>'*30}Data Valdaition log completed.{'<<'*30} \\n\\n\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc95751",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline.py file --> banking/pipeline dir\n",
    "from housing.component.data_validation import DataValidation\n",
    "from housing.entity.artifact_entity import DataIngestionArtifact, DataValidationArtifact\n",
    "\n",
    "def start_data_validation(self, data_ingestion_artifact: DataIngestionArtifact) \\\n",
    "        -> DataValidationArtifact:\n",
    "    try:\n",
    "        data_validation = DataValidation(data_validation_config=self.config.get_data_validation_config(),\n",
    "                                         data_ingestion_artifact=data_ingestion_artifact\n",
    "                                         )\n",
    "        return data_validation.initiate_data_validation()\n",
    "    except Exception as e:\n",
    "        raise HousingException(e, sys) from e\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d31e802",
   "metadata": {},
   "source": [
    "## 3. DataTransformation only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9332dcff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# config.yaml file\n",
    "\n",
    "data_transformation_config:\n",
    "  add_bedroom_per_room: true\n",
    "  transformed_dir: transformed_data\n",
    "  transformed_train_dir: train\n",
    "  transformed_test_dir: test\n",
    "  preprocessing_dir: preprocessed\n",
    "  preprocessed_object_file_name: preprocessed.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1979a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# constant file\n",
    "\n",
    "# Data Transformation related variables\n",
    "DATA_TRANSFORMATION_ARTIFACT_DIR = \"data_transformation\"\n",
    "DATA_TRANSFORMATION_CONFIG_KEY = \"data_transformation_config\"\n",
    "DATA_TRANSFORMATION_ADD_BEDROOM_PER_ROOM_KEY = \"add_bedroom_per_room\"\n",
    "DATA_TRANSFORMATION_DIR_NAME_KEY = \"transformed_dir\"\n",
    "DATA_TRANSFORMATION_TRAIN_DIR_NAME_KEY = \"transformed_train_dir\"\n",
    "DATA_TRANSFORMATION_TEST_DIR_NAME_KEY = \"transformed_test_dir\"\n",
    "DATA_TRANSFORMATION_PREPROCESSING_DIR_KEY = \"preprocessing_dir\"\n",
    "DATA_TRANSFORMATION_PREPROCESSED_FILE_NAME_KEY = \"preprocessed_object_file_name\"\n",
    "\n",
    "\n",
    "COLUMN_TOTAL_ROOMS = \"total_rooms\"\n",
    "COLUMN_POPULATION = \"population\"\n",
    "COLUMN_HOUSEHOLDS = \"households\"\n",
    "COLUMN_TOTAL_BEDROOM = \"total_bedrooms\"\n",
    "DATASET_SCHEMA_COLUMNS_KEY=  \"columns\"\n",
    "\n",
    "NUMERICAL_COLUMN_KEY=\"numerical_columns\"\n",
    "CATEGORICAL_COLUMN_KEY = \"categorical_columns\"\n",
    "\n",
    "TARGET_COLUMN_KEY=\"target_column\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3baa920",
   "metadata": {},
   "outputs": [],
   "source": [
    "# config_entity file from banking/enitity dir\n",
    "DataTransformationConfig = namedtuple(\"DataTransformationConfig\", [\"add_bedroom_per_room\",\n",
    "                                                                   \"transformed_train_dir\",\n",
    "                                                                   \"transformed_test_dir\",\n",
    "                                                                   \"preprocessed_object_file_path\"])\n",
    "\n",
    "# artifact_entity file from banking/enitity dir\n",
    "\n",
    "DataTransformationArtifact = namedtuple(\"DataTransformationArtifact\",\n",
    " [\"is_transformed\", \"message\", \"transformed_train_file_path\",\"transformed_test_file_path\",\n",
    "     \"preprocessed_object_file_path\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc8a904",
   "metadata": {},
   "outputs": [],
   "source": [
    "# util.py file from banking/util dir \n",
    "\n",
    "import yaml\n",
    "from housing.exception import HousingException\n",
    "import os,sys\n",
    "import numpy as np\n",
    "import dill\n",
    "import pandas as pd\n",
    "from housing.constant import *\n",
    "\n",
    "\n",
    "def write_yaml_file(file_path:str,data:dict=None):\n",
    "    \"\"\"\n",
    "    Create yaml file \n",
    "    file_path: str\n",
    "    data: dict\n",
    "    \"\"\"\n",
    "    try:\n",
    "        os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
    "        with open(file_path,\"w\") as yaml_file:\n",
    "            if data is not None:\n",
    "                yaml.dump(data,yaml_file)\n",
    "    except Exception as e:\n",
    "        raise HousingException(e,sys)\n",
    "\n",
    "\n",
    "def read_yaml_file(file_path:str)->dict:\n",
    "    \"\"\"\n",
    "    Reads a YAML file and returns the contents as a dictionary.\n",
    "    file_path: str\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'rb') as yaml_file:\n",
    "            return yaml.safe_load(yaml_file)\n",
    "    except Exception as e:\n",
    "        raise HousingException(e,sys) from e\n",
    "\n",
    "\n",
    "def save_numpy_array_data(file_path: str, array: np.array):\n",
    "    \"\"\"\n",
    "    Save numpy array data to file\n",
    "    file_path: str location of file to save\n",
    "    array: np.array data to save\n",
    "    \"\"\"\n",
    "    try:\n",
    "        dir_path = os.path.dirname(file_path)\n",
    "        os.makedirs(dir_path, exist_ok=True)\n",
    "        with open(file_path, 'wb') as file_obj:\n",
    "            np.save(file_obj, array)\n",
    "    except Exception as e:\n",
    "        raise HousingException(e, sys) from e\n",
    "\n",
    "\n",
    "def load_numpy_array_data(file_path: str) -> np.array:\n",
    "    \"\"\"\n",
    "    load numpy array data from file\n",
    "    file_path: str location of file to load\n",
    "    return: np.array data loaded\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'rb') as file_obj:\n",
    "            return np.load(file_obj)\n",
    "    except Exception as e:\n",
    "        raise HousingException(e, sys) from e\n",
    "\n",
    "\n",
    "def save_object(file_path:str,obj):\n",
    "    \"\"\"\n",
    "    file_path: str\n",
    "    obj: Any sort of object\n",
    "    \"\"\"\n",
    "    try:\n",
    "        dir_path = os.path.dirname(file_path)\n",
    "        os.makedirs(dir_path, exist_ok=True)\n",
    "        with open(file_path, \"wb\") as file_obj:\n",
    "            dill.dump(obj, file_obj)\n",
    "    except Exception as e:\n",
    "        raise HousingException(e,sys) from e\n",
    "\n",
    "\n",
    "def load_object(file_path:str):\n",
    "    \"\"\"\n",
    "    file_path: str\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(file_path, \"rb\") as file_obj:\n",
    "            return dill.load(file_obj)\n",
    "    except Exception as e:\n",
    "        raise HousingException(e,sys) from e\n",
    "\n",
    "\n",
    "def load_data(file_path: str, schema_file_path: str) -> pd.DataFrame:\n",
    "    try:\n",
    "        datatset_schema = read_yaml_file(schema_file_path)\n",
    "\n",
    "        schema = datatset_schema[DATASET_SCHEMA_COLUMNS_KEY]\n",
    "\n",
    "        dataframe = pd.read_csv(file_path)\n",
    "\n",
    "        error_messgae = \"\"\n",
    "\n",
    "\n",
    "        for column in dataframe.columns:\n",
    "            if column in list(schema.keys()):\n",
    "                dataframe[column].astype(schema[column])\n",
    "            else:\n",
    "                error_messgae = f\"{error_messgae} \\nColumn: [{column}] is not in the schema.\"\n",
    "        if len(error_messgae) > 0:\n",
    "            raise Exception(error_messgae)\n",
    "        return dataframe\n",
    "\n",
    "    except Exception as e:\n",
    "        raise HousingException(e,sys) from e\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124db9eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# configuration.py file from banking/config dir\n",
    "\n",
    "def get_data_transformation_config(self) -> DataTransformationConfig:\n",
    "    try:\n",
    "        artifact_dir = self.training_pipeline_config.artifact_dir\n",
    "\n",
    "        data_transformation_artifact_dir=os.path.join(\n",
    "            artifact_dir,\n",
    "            DATA_TRANSFORMATION_ARTIFACT_DIR,\n",
    "            self.time_stamp\n",
    "        )\n",
    "\n",
    "        data_transformation_config_info=self.config_info[DATA_TRANSFORMATION_CONFIG_KEY]\n",
    "\n",
    "        add_bedroom_per_room=data_transformation_config_info[DATA_TRANSFORMATION_ADD_BEDROOM_PER_ROOM_KEY]\n",
    "\n",
    "\n",
    "        preprocessed_object_file_path = os.path.join(\n",
    "            data_transformation_artifact_dir,\n",
    "            data_transformation_config_info[DATA_TRANSFORMATION_PREPROCESSING_DIR_KEY],\n",
    "            data_transformation_config_info[DATA_TRANSFORMATION_PREPROCESSED_FILE_NAME_KEY]\n",
    "        )\n",
    "\n",
    "\n",
    "        transformed_train_dir=os.path.join(\n",
    "        data_transformation_artifact_dir,\n",
    "        data_transformation_config_info[DATA_TRANSFORMATION_DIR_NAME_KEY],\n",
    "        data_transformation_config_info[DATA_TRANSFORMATION_TRAIN_DIR_NAME_KEY]\n",
    "        )\n",
    "\n",
    "\n",
    "        transformed_test_dir = os.path.join(\n",
    "        data_transformation_artifact_dir,\n",
    "        data_transformation_config_info[DATA_TRANSFORMATION_DIR_NAME_KEY],\n",
    "        data_transformation_config_info[DATA_TRANSFORMATION_TEST_DIR_NAME_KEY]\n",
    "\n",
    "        )\n",
    "\n",
    "\n",
    "        data_transformation_config=DataTransformationConfig(\n",
    "            add_bedroom_per_room=add_bedroom_per_room,\n",
    "            preprocessed_object_file_path=preprocessed_object_file_path,\n",
    "            transformed_train_dir=transformed_train_dir,\n",
    "            transformed_test_dir=transformed_test_dir\n",
    "        )\n",
    "\n",
    "        logging.info(f\"Data transformation config: {data_transformation_config}\")\n",
    "        return data_transformation_config\n",
    "    except Exception as e:\n",
    "        raise HousingException(e,sys) from e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec653eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data_transformation file from banking/component dir\n",
    "\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from housing.exception import HousingException\n",
    "from housing.logger import logging\n",
    "from housing.entity.config_entity import DataTransformationConfig \n",
    "from housing.entity.artifact_entity import DataIngestionArtifact,\\\n",
    "DataValidationArtifact,DataTransformationArtifact\n",
    "import sys,os\n",
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator,TransformerMixin\n",
    "from sklearn.preprocessing import StandardScaler,OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "import pandas as pd\n",
    "from housing.constant import *\n",
    "from housing.util.util import read_yaml_file,save_object,save_numpy_array_data,load_data\n",
    "\n",
    "\n",
    "#   longitude: float\n",
    "#   latitude: float\n",
    "#   housing_median_age: float\n",
    "#   total_rooms: float\n",
    "#   total_bedrooms: float\n",
    "#   population: float\n",
    "#   households: float\n",
    "#   median_income: float\n",
    "#   median_house_value: float\n",
    "#   ocean_proximity: category\n",
    "#   income_cat: float\n",
    "\n",
    "\n",
    "class FeatureGenerator(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self, add_bedrooms_per_room=True,\n",
    "                 total_rooms_ix=3,\n",
    "                 population_ix=5,\n",
    "                 households_ix=6,\n",
    "                 total_bedrooms_ix=4, columns=None):\n",
    "        \"\"\"\n",
    "        FeatureGenerator Initialization\n",
    "        add_bedrooms_per_room: bool\n",
    "        total_rooms_ix: int index number of total rooms columns\n",
    "        population_ix: int index number of total population columns\n",
    "        households_ix: int index number of  households columns\n",
    "        total_bedrooms_ix: int index number of bedrooms columns\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.columns = columns\n",
    "            if self.columns is not None:\n",
    "                total_rooms_ix = self.columns.index(COLUMN_TOTAL_ROOMS)\n",
    "                population_ix = self.columns.index(COLUMN_POPULATION)\n",
    "                households_ix = self.columns.index(COLUMN_HOUSEHOLDS)\n",
    "                total_bedrooms_ix = self.columns.index(COLUMN_TOTAL_BEDROOM)\n",
    "\n",
    "            self.add_bedrooms_per_room = add_bedrooms_per_room\n",
    "            self.total_rooms_ix = total_rooms_ix\n",
    "            self.population_ix = population_ix\n",
    "            self.households_ix = households_ix\n",
    "            self.total_bedrooms_ix = total_bedrooms_ix\n",
    "        except Exception as e:\n",
    "            raise HousingException(e, sys) from e\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        try:\n",
    "            room_per_household = X[:, self.total_rooms_ix] / \\\n",
    "                                 X[:, self.households_ix]\n",
    "            population_per_household = X[:, self.population_ix] / \\\n",
    "                                       X[:, self.households_ix]\n",
    "            if self.add_bedrooms_per_room:\n",
    "                bedrooms_per_room = X[:, self.total_bedrooms_ix] / \\\n",
    "                                    X[:, self.total_rooms_ix]\n",
    "                generated_feature = np.c_[\n",
    "                    X, room_per_household, population_per_household, bedrooms_per_room]\n",
    "            else:\n",
    "                generated_feature = np.c_[\n",
    "                    X, room_per_household, population_per_household]\n",
    "\n",
    "            return generated_feature\n",
    "        except Exception as e:\n",
    "            raise HousingException(e, sys) from e\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class DataTransformation:\n",
    "\n",
    "    def __init__(self, data_transformation_config: DataTransformationConfig,\n",
    "                 data_ingestion_artifact: DataIngestionArtifact,\n",
    "                 data_validation_artifact: DataValidationArtifact\n",
    "                 ):\n",
    "        try:\n",
    "            logging.info(f\"{'>>' * 30}Data Transformation log started.{'<<' * 30} \")\n",
    "            self.data_transformation_config= data_transformation_config\n",
    "            self.data_ingestion_artifact = data_ingestion_artifact\n",
    "            self.data_validation_artifact = data_validation_artifact\n",
    "\n",
    "        except Exception as e:\n",
    "            raise HousingException(e,sys) from e\n",
    "\n",
    "    \n",
    "\n",
    "    def get_data_transformer_object(self)->ColumnTransformer:\n",
    "        try:\n",
    "            schema_file_path = self.data_validation_artifact.schema_file_path\n",
    "\n",
    "            dataset_schema = read_yaml_file(file_path=schema_file_path)\n",
    "\n",
    "            numerical_columns = dataset_schema[NUMERICAL_COLUMN_KEY]\n",
    "            categorical_columns = dataset_schema[CATEGORICAL_COLUMN_KEY]\n",
    "\n",
    "\n",
    "            num_pipeline = Pipeline(steps=[\n",
    "                ('imputer', SimpleImputer(strategy=\"median\")),\n",
    "                ('feature_generator', FeatureGenerator(\n",
    "                    add_bedrooms_per_room=self.data_transformation_config.add_bedroom_per_room,\n",
    "                    columns=numerical_columns\n",
    "                )),\n",
    "                ('scaler', StandardScaler())\n",
    "            ]\n",
    "            )\n",
    "\n",
    "            cat_pipeline = Pipeline(steps=[\n",
    "                 ('impute', SimpleImputer(strategy=\"most_frequent\")),\n",
    "                 ('one_hot_encoder', OneHotEncoder()),\n",
    "                 ('scaler', StandardScaler(with_mean=False))\n",
    "            ]\n",
    "            )\n",
    "\n",
    "            logging.info(f\"Categorical columns: {categorical_columns}\")\n",
    "            logging.info(f\"Numerical columns: {numerical_columns}\")\n",
    "\n",
    "\n",
    "            preprocessing = ColumnTransformer([\n",
    "                ('num_pipeline', num_pipeline, numerical_columns),\n",
    "                ('cat_pipeline', cat_pipeline, categorical_columns),\n",
    "            ])\n",
    "            return preprocessing\n",
    "\n",
    "        except Exception as e:\n",
    "            raise HousingException(e,sys) from e   \n",
    "\n",
    "\n",
    "    def initiate_data_transformation(self)->DataTransformationArtifact:\n",
    "        try:\n",
    "            logging.info(f\"Obtaining preprocessing object.\")\n",
    "            preprocessing_obj = self.get_data_transformer_object()\n",
    "\n",
    "\n",
    "            logging.info(f\"Obtaining training and test file path.\")\n",
    "            train_file_path = self.data_ingestion_artifact.train_file_path\n",
    "            test_file_path = self.data_ingestion_artifact.test_file_path\n",
    "            \n",
    "\n",
    "            schema_file_path = self.data_validation_artifact.schema_file_path\n",
    "            \n",
    "            logging.info(f\"Loading training and test data as pandas dataframe.\")\n",
    "            train_df = load_data(file_path=train_file_path, schema_file_path=schema_file_path)\n",
    "            \n",
    "            test_df = load_data(file_path=test_file_path, schema_file_path=schema_file_path)\n",
    "\n",
    "            schema = read_yaml_file(file_path=schema_file_path)\n",
    "\n",
    "            target_column_name = schema[TARGET_COLUMN_KEY]\n",
    "\n",
    "\n",
    "            logging.info(f\"Splitting input and target feature from training and testing dataframe.\")\n",
    "            input_feature_train_df = train_df.drop(columns=[target_column_name],axis=1)\n",
    "            target_feature_train_df = train_df[target_column_name]\n",
    "\n",
    "            input_feature_test_df = test_df.drop(columns=[target_column_name],axis=1)\n",
    "            target_feature_test_df = test_df[target_column_name]\n",
    "            \n",
    "\n",
    "            logging.info(f\"Applying preprocessing object on training dataframe and testing dataframe\")\n",
    "            input_feature_train_arr=preprocessing_obj.fit_transform(input_feature_train_df)\n",
    "            input_feature_test_arr = preprocessing_obj.transform(input_feature_test_df)\n",
    "\n",
    "\n",
    "            train_arr = np.c_[ input_feature_train_arr, np.array(target_feature_train_df)]\n",
    "\n",
    "            test_arr = np.c_[input_feature_test_arr, np.array(target_feature_test_df)]\n",
    "            \n",
    "            transformed_train_dir = self.data_transformation_config.transformed_train_dir\n",
    "            transformed_test_dir = self.data_transformation_config.transformed_test_dir\n",
    "\n",
    "            train_file_name = os.path.basename(train_file_path).replace(\".csv\",\".npz\")\n",
    "            test_file_name = os.path.basename(test_file_path).replace(\".csv\",\".npz\")\n",
    "\n",
    "            transformed_train_file_path = os.path.join(transformed_train_dir, train_file_name)\n",
    "            transformed_test_file_path = os.path.join(transformed_test_dir, test_file_name)\n",
    "\n",
    "            logging.info(f\"Saving transformed training and testing array.\")\n",
    "            \n",
    "            save_numpy_array_data(file_path=transformed_train_file_path,array=train_arr)\n",
    "            save_numpy_array_data(file_path=transformed_test_file_path,array=test_arr)\n",
    "\n",
    "            preprocessing_obj_file_path = self.data_transformation_config.preprocessed_object_file_path\n",
    "\n",
    "            logging.info(f\"Saving preprocessing object.\")\n",
    "            save_object(file_path=preprocessing_obj_file_path,obj=preprocessing_obj)\n",
    "\n",
    "            data_transformation_artifact = DataTransformationArtifact(is_transformed=True,\n",
    "            message=\"Data transformation successfull.\",\n",
    "            transformed_train_file_path=transformed_train_file_path,\n",
    "            transformed_test_file_path=transformed_test_file_path,\n",
    "            preprocessed_object_file_path=preprocessing_obj_file_path\n",
    "\n",
    "            )\n",
    "            logging.info(f\"Data transformationa artifact: {data_transformation_artifact}\")\n",
    "            return data_transformation_artifact\n",
    "        except Exception as e:\n",
    "            raise HousingException(e,sys) from e\n",
    "\n",
    "    def __del__(self):\n",
    "        logging.info(f\"{'>>'*30}Data Transformation log completed.{'<<'*30} \\n\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a5c9b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline fro datatrasfromation\n",
    "\n",
    "\n",
    "def start_data_transformation(self,\n",
    "                              data_ingestion_artifact: DataIngestionArtifact,\n",
    "                              data_validation_artifact: DataValidationArtifact\n",
    "                              ) -> DataTransformationArtifact:\n",
    "    try:\n",
    "        data_transformation = DataTransformation(\n",
    "            data_transformation_config=self.config.get_data_transformation_config(),\n",
    "            data_ingestion_artifact=data_ingestion_artifact,\n",
    "            data_validation_artifact=data_validation_artifact\n",
    "        )\n",
    "        return data_transformation.initiate_data_transformation()\n",
    "    except Exception as e:\n",
    "        raise HousingException(e, sys)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a555643",
   "metadata": {},
   "source": [
    "## 4.Model training only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b855ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f74b208",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07962e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "DataValidationArtifact = \n",
    "namedtuple(\"DataValidationArtifact\",[\"schema_file_path\",\"report_file_path\",\"report_page_file_path\",\"is_validated\",\"message\"])\n",
    "namedtuple(\"DataValidationConfig\",[\"schema_file_path\",\"report_file_path\",\"report_page_file_path\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db92368",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
